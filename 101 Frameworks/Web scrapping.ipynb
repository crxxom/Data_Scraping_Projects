{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328d39cc",
   "metadata": {},
   "source": [
    "# Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3924d9e",
   "metadata": {},
   "source": [
    "TechwithTime T3 Navigating the HTML tree\n",
    "https://www.youtube.com/watch?v=lC6mucyD17k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674a189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://coinmarketcap.com/'\n",
    "result = requests.get(url).text\n",
    "doc = BeautifulSoup(result, 'html.parser')\n",
    "\n",
    "tbody = doc.tbody #in the website all price info is stored inside a huge table call tbody\n",
    "trs = tbody.contents #grab content inside the table, returning a list\n",
    "\n",
    "#Traversing different levels\n",
    "trs[1].previous_sibling #return trs[0]\n",
    "trs[1].next_sibling #return trs[2]\n",
    "trs[1].next_siblings #return a generator object trs[1:]\n",
    "\n",
    "trs[0].parent.name #tbody\n",
    "list(trs[0].descendants)\n",
    "list(trs[0].contents)\n",
    "list(trs[0].children)\n",
    "\n",
    "#Getting crytoprices\n",
    "prices = {}\n",
    "\n",
    "for tr in trs[:10]:\n",
    "    name, price = tr.contents[2:4]\n",
    "    fixed_name = name.p.string\n",
    "    fixed_price = price.a.get_text(strip=True)\n",
    "    prices[fixed_name] = fixed_price\n",
    "    \n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573c31c",
   "metadata": {},
   "source": [
    "TechwithTime T4 Finding the Best GPU price https://www.youtube.com/watch?v=zAEfWiC_KBU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "search_term = input(\"What product do you want to search for: \")\n",
    "url = f\"https://www.newegg.com/p/pl?d={search_term}&N=4131\"\n",
    "page = requests.get(url).text\n",
    "doc = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "page_text = doc.find(class_='list-tool-pagination-text').strong\n",
    "pages = int(str(page_text).split('/')[-2].split('>')[-1][:-1])\n",
    "\n",
    "items_found = {}\n",
    "\n",
    "for page in range(1, pages + 1):\n",
    "    url = f\"https://www.newegg.com/p/pl?d={search_term}&N=4131&page={page}\"\n",
    "    page = requests.get(url).text\n",
    "    doc = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    div = doc.find(class_='item-cells-wrap border-cells items-grid-view four-cells expulsion-one-cell')\n",
    "    items = div.find_all(text=re.compile(search_term))\n",
    "    for item in items:\n",
    "        parent = item.parent\n",
    "        if parent.name != \"a\":\n",
    "            continue\n",
    "            \n",
    "        link = parent['href']\n",
    "        next_parent = item.find_parent(class_=\"item-container\")\n",
    "        try:\n",
    "            price = next_parent.find(class_=\"price-current\").find('strong').string\n",
    "            items_found[item] = {'price': int(price.replace(',','')), 'link': link}\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "sorted_items = sorted(items_found.items(), key=lambda x: x[1]['price'])\n",
    "\n",
    "# for item in sorted_items:\n",
    "#     print(item[0])\n",
    "#     print(f\"${item[1]['price']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78dd91",
   "metadata": {},
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ea173",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48537de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "driver.get(\"https://www.neuralnine.com/\")\n",
    "driver.maximize_window()\n",
    "print(driver.title)\n",
    "\n",
    "links = driver.find_elements(\"xpath\", \"//a[@href]\")\n",
    "for link in links:\n",
    "    if \"Books\" in link.get_attribute(\"innerHTML\"):\n",
    "        link.click()\n",
    "        break\n",
    "\n",
    "book_links = driver.find_elements(\"xpath\", \"//div[contains(@class, 'elementor-column-wrap')][.//h2[text()[contains(.,'7 IN 1')]]][count(.//a)=2]//a\")\n",
    "\n",
    "book_links[0].click()\n",
    "driver.switch_to.window(driver.window_handles[1]) #switch to 2nd tab\n",
    "\n",
    "time.sleep(5)\n",
    "buttons = driver.find_elements(\"xpath\", \"//a[.//span[text()[contains(.,'Tascjenbuch')]]]//span[text()[contains(.,$)]]\")\n",
    "for button in buttons:\n",
    "    print(button.get_attribute(\"innerHTML\"))\n",
    "driver.quit()\n",
    "\n",
    "search = driver.find_element(\"name\", \"s\")\n",
    "search.send_keys(\"test\")\n",
    "search.send_keys(Keys.RETURN)\n",
    "\n",
    "print(driver.page_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "options = Options()\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "driver.get(\"https://anilist.co/search/manga?format=MANGA&publishing%20status=FINISHED&country%20of%20origin=KR&sort=SCORE_DESC&year=2022\")\n",
    "\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "print(last_height)\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    print(\"ran\", end='')\n",
    "    time.sleep(10)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    print(new_height)\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    else:\n",
    "        last_height = new_height\n",
    "\n",
    "doc = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "len(doc.find_all(class_=\"media-card\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42816d9b",
   "metadata": {},
   "source": [
    "# XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea13303",
   "metadata": {},
   "source": [
    "Test your xpath query: http://xpather.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a415185",
   "metadata": {},
   "source": [
    "**Absolute referencing:** '/' eg, body/div (basically only search all div under 1 indent of body <br>\n",
    "**Relative referencing:** '//' eg, body//div (return all the div tags under body)\n",
    "\n",
    "\n",
    "**Self-reference:** '.' eg, //a/./@title (basically grab the 'title' attribute inside that a tag)<br>\n",
    "**Parent selection:** '..' eg, //a/../@class (get the 'class' of the parent of the a tag)\n",
    "\n",
    "**Grabbing text:** 'text()' eg, //a/text() (get the text inside the a tag)<br>\n",
    "**Grabbing attributes:** '/@atr' eg, //a/@href (get the href inside the a tag)<br>\n",
    "\n",
    "\n",
    "**Filtering:** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bba12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "//a[@title]/@href \"get the links of all <a> tag that contains a title\"\n",
    "\n",
    "a[*] \"return <a> tag that have something in them that isn't just text, eg. img\"\n",
    "\n",
    "//*[@*] \"select everything that contains something as attribute\"\n",
    "\n",
    "//div[@class='col-sm-8 h1'] 'select all div that has the exact class of \"col-sm-8 h1'\n",
    "\n",
    "//div[contains(@class, 'col-sm-8')] 'select all div that contains the class \"col-sm-8\"'\n",
    "\n",
    "//*[text()[contains(., 'Book')]] 'find any element that contains the text \"book\"'\n",
    "\n",
    "//*[count(.//li)=20] 'return the item that contains 20 list items in its children'\n",
    "\n",
    "//div/following-sibling::div 'return div block that is after another div block in its scope'\n",
    "//div/preceding-sibling::div 'return div block that is before another div block in its scope'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96162fd",
   "metadata": {},
   "source": [
    "# Automation with python - freecodecamp\n",
    "\n",
    "Youtube: https://www.youtube.com/watch?v=PXMJ6FS7llk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9011d",
   "metadata": {},
   "source": [
    "## Extracting table from websites using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# use read_html to return all tables from the website\n",
    "simpsons = pd.read_html('https://en.wikipedia.org/wiki/List_of_The_Simpsons_episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655c15d",
   "metadata": {},
   "source": [
    "## Extracting table from PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tk\n",
    "pip install ghostscript\n",
    "pip install camelot-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98510e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "\n",
    "tables = camelot.read_pdf('foo.pdf', pages='1')\n",
    "tables[0].export('foo.csv', f='csv', compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee201b",
   "metadata": {},
   "source": [
    "## Seleniem project - Daily auto execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ce99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Preparing script before we convert it to executable\n",
    "application_path = os.path.dirname(sys.executable)\n",
    "\n",
    "# get date in format MMDDYYYY\n",
    "now = datetime.now()\n",
    "month_day_year = now.strftime(\"%m%d%Y\")\n",
    "\n",
    "web = 'https://www.thesun.co.uk/sport/football/'\n",
    "path = '/Users/frankandrade/Downloads/chromedriver'  # introduce path here\n",
    "\n",
    "# Headless mode\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver_service = Service(executable_path=path)\n",
    "driver = webdriver.Chrome(service=driver_service, options=options)\n",
    "driver.get(web)\n",
    "\n",
    "containers = driver.find_elements(by='xpath', value='//div[@class=\"teaser__copy-container\"]')\n",
    "\n",
    "titles = []\n",
    "subtitles = []\n",
    "links = []\n",
    "for container in containers:\n",
    "    title = container.find_element(by='xpath', value='./a/h2').text\n",
    "    subtitle = container.find_element(by='xpath', value='./a/p').text\n",
    "    link = container.find_element(by='xpath', value='./a').get_attribute('href')\n",
    "    titles.append(title)\n",
    "    subtitles.append(subtitle)\n",
    "    links.append(link)\n",
    "\n",
    "# Exporting data to the same folder where the executable will be located\n",
    "my_dict = {'title': titles, 'subtitle': subtitles, 'link': links}\n",
    "df_headlines = pd.DataFrame(my_dict)\n",
    "file_name = f'football_headlines_{month_day_year}.csv'\n",
    "final_path = os.path.join(application_path, file_name)\n",
    "df_headlines.to_csv(final_path)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341fba97",
   "metadata": {},
   "source": [
    "To convert the script to be able to auto execute daily, you need to run it as python file and convert it to exe. Follow the following steps (make sure your current directory is in the project directory [able to see the file])\n",
    "\n",
    "1. `pip install pyinstaller`\n",
    "2. `pyinstaller --onefile *name of py file*`\n",
    "\n",
    "After doing the two steps you should see build and dict folder in your directory, the exe is the one you see inside dict folder. If you want to run it, right click it and open with terminal.\n",
    "\n",
    "Now we want it to run it automatically daily\n",
    "\n",
    "1. `crontab -e`\n",
    "2. You can test the cron schedule at https://crontab.guru/\n",
    "3. Go back to the terminal, for example, if you want it to run in 9am everyday, paste 09\\*\\*\\* in the vim editor and press **i**  -> drag the exe file into the terminal to get the path of the exe file (a space between 09\\*\\*\\* and the path of exe). Then press **esc** and **:wq**\n",
    "4. `contab -l` to see if the command is created"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
