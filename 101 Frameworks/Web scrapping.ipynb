{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328d39cc",
   "metadata": {},
   "source": [
    "# Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3924d9e",
   "metadata": {},
   "source": [
    "TechwithTime T3 Navigating the HTML tree\n",
    "https://www.youtube.com/watch?v=lC6mucyD17k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674a189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://coinmarketcap.com/'\n",
    "result = requests.get(url).text\n",
    "doc = BeautifulSoup(result, 'html.parser')\n",
    "\n",
    "tbody = doc.tbody #in the website all price info is stored inside a huge table call tbody\n",
    "trs = tbody.contents #grab content inside the table, returning a list\n",
    "\n",
    "#Traversing different levels\n",
    "trs[1].previous_sibling #return trs[0]\n",
    "trs[1].next_sibling #return trs[2]\n",
    "trs[1].next_siblings #return a generator object trs[1:]\n",
    "\n",
    "trs[0].parent.name #tbody\n",
    "list(trs[0].descendants)\n",
    "list(trs[0].contents)\n",
    "list(trs[0].children)\n",
    "\n",
    "#Getting crytoprices\n",
    "prices = {}\n",
    "\n",
    "for tr in trs[:10]:\n",
    "    name, price = tr.contents[2:4]\n",
    "    fixed_name = name.p.string\n",
    "    fixed_price = price.a.get_text(strip=True)\n",
    "    prices[fixed_name] = fixed_price\n",
    "    \n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573c31c",
   "metadata": {},
   "source": [
    "TechwithTime T4 Finding the Best GPU price https://www.youtube.com/watch?v=zAEfWiC_KBU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "search_term = input(\"What product do you want to search for: \")\n",
    "url = f\"https://www.newegg.com/p/pl?d={search_term}&N=4131\"\n",
    "page = requests.get(url).text\n",
    "doc = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "page_text = doc.find(class_='list-tool-pagination-text').strong\n",
    "pages = int(str(page_text).split('/')[-2].split('>')[-1][:-1])\n",
    "\n",
    "items_found = {}\n",
    "\n",
    "for page in range(1, pages + 1):\n",
    "    url = f\"https://www.newegg.com/p/pl?d={search_term}&N=4131&page={page}\"\n",
    "    page = requests.get(url).text\n",
    "    doc = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    div = doc.find(class_='item-cells-wrap border-cells items-grid-view four-cells expulsion-one-cell')\n",
    "    items = div.find_all(text=re.compile(search_term))\n",
    "    for item in items:\n",
    "        parent = item.parent\n",
    "        if parent.name != \"a\":\n",
    "            continue\n",
    "            \n",
    "        link = parent['href']\n",
    "        next_parent = item.find_parent(class_=\"item-container\")\n",
    "        try:\n",
    "            price = next_parent.find(class_=\"price-current\").find('strong').string\n",
    "            items_found[item] = {'price': int(price.replace(',','')), 'link': link}\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "sorted_items = sorted(items_found.items(), key=lambda x: x[1]['price'])\n",
    "\n",
    "# for item in sorted_items:\n",
    "#     print(item[0])\n",
    "#     print(f\"${item[1]['price']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78dd91",
   "metadata": {},
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ea173",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48537de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "driver.get(\"https://www.neuralnine.com/\")\n",
    "driver.maximize_window()\n",
    "print(driver.title)\n",
    "\n",
    "links = driver.find_elements(\"xpath\", \"//a[@href]\")\n",
    "for link in links:\n",
    "    if \"Books\" in link.get_attribute(\"innerHTML\"):\n",
    "        link.click()\n",
    "        break\n",
    "\n",
    "book_links = driver.find_elements(\"xpath\", \"//div[contains(@class, 'elementor-column-wrap')][.//h2[text()[contains(.,'7 IN 1')]]][count(.//a)=2]//a\")\n",
    "\n",
    "book_links[0].click()\n",
    "driver.switch_to.window(driver.window_handles[1]) #switch to 2nd tab\n",
    "\n",
    "time.sleep(5)\n",
    "buttons = driver.find_elements(\"xpath\", \"//a[.//span[text()[contains(.,'Tascjenbuch')]]]//span[text()[contains(.,$)]]\")\n",
    "for button in buttons:\n",
    "    print(button.get_attribute(\"innerHTML\"))\n",
    "driver.quit()\n",
    "\n",
    "search = driver.find_element(\"name\", \"s\")\n",
    "search.send_keys(\"test\")\n",
    "search.send_keys(Keys.RETURN)\n",
    "\n",
    "print(driver.page_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f51848-911c-40ca-bba3-99f21b56be22",
   "metadata": {},
   "source": [
    "### Important technique using Selenium -> driver.execute(js code) to do dynamic scraping like scrolling down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15471b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "options = Options()\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "driver.get(\"https://anilist.co/search/manga?format=MANGA&publishing%20status=FINISHED&country%20of%20origin=KR&sort=SCORE_DESC&year=2022\")\n",
    "\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "print(last_height)\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    print(\"ran\", end='')\n",
    "    time.sleep(10)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    print(new_height)\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    else:\n",
    "        last_height = new_height\n",
    "\n",
    "doc = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027cd0f7-0117-40dd-add8-8fc53e20834a",
   "metadata": {},
   "source": [
    "## Important Selenium Fix for Chrome version 115+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2e01c-86b9-4b81-9090-e68ad3a10da1",
   "metadata": {},
   "source": [
    "Since chrome version 114+, WebDriverManager has migrated it's install place and stuff has messed up so you can't do the normal stuff normally, here is some fix I found online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247fd78-f2ce-4f66-a663-6772ade5328e",
   "metadata": {},
   "source": [
    "First fix is to add service = Service() line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3a5b2-5c8f-4fdf-a5c8-c13d91f7088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "service = Service() # this is the important line of code!\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless=new\")\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "driver.get('https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx1YlY4U0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US%3Aen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969970e-35fc-4637-9597-787d89a10a5b",
   "metadata": {},
   "source": [
    "2nd fix - specify a lower version of chrome in selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4ff43-9172-4c65-87e3-3667cd33a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager(version=\"114.0.5735.90\").install()),\n",
    "                         options=option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42816d9b",
   "metadata": {},
   "source": [
    "# XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea13303",
   "metadata": {},
   "source": [
    "Test your xpath query: http://xpather.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a415185",
   "metadata": {},
   "source": [
    "**Absolute referencing:** '/' eg, body/div (basically only search all div under 1 indent of body <br>\n",
    "**Relative referencing:** '//' eg, body//div (return all the div tags under body)\n",
    "\n",
    "\n",
    "**Self-reference:** '.' eg, //a/./@title (basically grab the 'title' attribute inside that a tag)<br>\n",
    "**Parent selection:** '..' eg, //a/../@class (get the 'class' of the parent of the a tag)\n",
    "\n",
    "**Grabbing text:** 'text()' eg, //a/text() (get the text inside the a tag)<br>\n",
    "**Grabbing attributes:** '/@atr' eg, //a/@href (get the href inside the a tag)<br>\n",
    "\n",
    "\n",
    "**Filtering:** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bba12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "//a[@title]/@href \"get the links of all <a> tag that contains a title\"\n",
    "\n",
    "a[*] \"return <a> tag that have something in them that isn't just text, eg. img\"\n",
    "\n",
    "//*[@*] \"select everything that contains something as attribute\"\n",
    "\n",
    "//div[@class='col-sm-8 h1'] 'select all div that has the exact class of \"col-sm-8 h1'\n",
    "\n",
    "//div[contains(@class, 'col-sm-8')] 'select all div that contains the class \"col-sm-8\"'\n",
    "\n",
    "//*[text()[contains(., 'Book')]] 'find any element that contains the text \"book\"'\n",
    "\n",
    "//*[count(.//li)=20] 'return the item that contains 20 list items in its children'\n",
    "\n",
    "//div/following-sibling::div 'return div block that is after another div block in its scope'\n",
    "//div/preceding-sibling::div 'return div block that is before another div block in its scope'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96162fd",
   "metadata": {},
   "source": [
    "# Automation with python - freecodecamp\n",
    "\n",
    "Youtube: https://www.youtube.com/watch?v=PXMJ6FS7llk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9011d",
   "metadata": {},
   "source": [
    "## Extracting table from websites using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# use read_html to return all tables from the website\n",
    "simpsons = pd.read_html('https://en.wikipedia.org/wiki/List_of_The_Simpsons_episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655c15d",
   "metadata": {},
   "source": [
    "## Extracting table from PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tk\n",
    "pip install ghostscript\n",
    "pip install camelot-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98510e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "\n",
    "tables = camelot.read_pdf('foo.pdf', pages='1')\n",
    "tables[0].export('foo.csv', f='csv', compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee201b",
   "metadata": {},
   "source": [
    "## Seleniem project - Daily auto execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ce99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Preparing script before we convert it to executable\n",
    "application_path = os.path.dirname(sys.executable)\n",
    "\n",
    "# get date in format MMDDYYYY\n",
    "now = datetime.now()\n",
    "month_day_year = now.strftime(\"%m%d%Y\")\n",
    "\n",
    "web = 'https://www.thesun.co.uk/sport/football/'\n",
    "path = '/Users/frankandrade/Downloads/chromedriver'  # introduce path here\n",
    "\n",
    "# Headless mode\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver_service = Service(executable_path=path)\n",
    "driver = webdriver.Chrome(service=driver_service, options=options)\n",
    "driver.get(web)\n",
    "\n",
    "containers = driver.find_elements(by='xpath', value='//div[@class=\"teaser__copy-container\"]')\n",
    "\n",
    "titles = []\n",
    "subtitles = []\n",
    "links = []\n",
    "for container in containers:\n",
    "    title = container.find_element(by='xpath', value='./a/h2').text\n",
    "    subtitle = container.find_element(by='xpath', value='./a/p').text\n",
    "    link = container.find_element(by='xpath', value='./a').get_attribute('href')\n",
    "    titles.append(title)\n",
    "    subtitles.append(subtitle)\n",
    "    links.append(link)\n",
    "\n",
    "# Exporting data to the same folder where the executable will be located\n",
    "my_dict = {'title': titles, 'subtitle': subtitles, 'link': links}\n",
    "df_headlines = pd.DataFrame(my_dict)\n",
    "file_name = f'football_headlines_{month_day_year}.csv'\n",
    "final_path = os.path.join(application_path, file_name)\n",
    "df_headlines.to_csv(final_path)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341fba97",
   "metadata": {},
   "source": [
    "To convert the script to be able to auto execute daily, you need to run it as python file and convert it to exe. Follow the following steps (make sure your current directory is in the project directory [able to see the file])\n",
    "\n",
    "1. `pip install pyinstaller`\n",
    "2. `pyinstaller --onefile *name of py file*`\n",
    "\n",
    "After doing the two steps you should see build and dict folder in your directory, the exe is the one you see inside dict folder. If you want to run it, right click it and open with terminal.\n",
    "\n",
    "Now we want it to run it automatically daily\n",
    "\n",
    "1. `crontab -e`\n",
    "2. You can test the cron schedule at https://crontab.guru/\n",
    "3. Go back to the terminal, for example, if you want it to run in 9am everyday, paste 09\\*\\*\\* in the vim editor and press **i** and a space  -> drag the exe file into the terminal to get the path of the exe file (a space between 09\\*\\*\\* and the path of exe). Then press **esc** and **:wq**\n",
    "4. `contab -l` to see if the command is created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b55c6-21d6-40fe-bd0e-51e9c578d327",
   "metadata": {},
   "source": [
    "### Important! Crontab is not recommended for mac due to the security system of macos, use launchd instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19bb02-acc7-4b15-b44b-66ae72354867",
   "metadata": {},
   "source": [
    "1. Create .plist file, create it by typing `touch com.scraping.googlenews.plist` in the terminal in this example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc8ba7-2ea8-4eb0-92cc-5d1954c01131",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n",
    "<plist version=\"1.0\">\n",
    "  <dict>\n",
    "    <key>Label</key>\n",
    "    <string>com.scraping.googlenews</string>\n",
    "    <key>ProgramArguments</key>\n",
    "    <array>\n",
    "      <string>/Users/jadonng/opt/anaconda3/bin/python</string>\n",
    "      <string>/Users/jadonng/Desktop/Computer_Science/Data_Scraping/Google_News/GNScraping_nocontent.py</string>\n",
    "    </array>\n",
    "    <key>RunAtLoad</key>\n",
    "    <true/>\n",
    "    <key>StandardOutPath</key>\n",
    "    <string>/Users/jadonng/Desktop/Computer_Science/Data_Scraping/Google_News/stdout.log</string>\n",
    "    <key>StandardErrorPath</key>\n",
    "    <string>/Users/jadonng/Desktop/Computer_Science/Data_Scraping/Google_News/stderror.log</string>\n",
    "    <key>StartCalendarInterval</key>\n",
    "    <dict>\n",
    "      <key>Hour</key>\n",
    "      <integer>13</integer>\n",
    "      <key>Minute</key>\n",
    "      <integer>16</integer>\n",
    "    </dict>\n",
    "  </dict>\n",
    "</plist>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7eebd8-238c-4f98-a27b-999ff8c27f52",
   "metadata": {},
   "source": [
    "2. Move the file into Library/LaunchAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540ca55-4670-4651-a226-c4eba2a7217b",
   "metadata": {},
   "source": [
    "3. In terminal, load the plist file by typing `launchctl load plistfilepath` (drag the plist file in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44156ee4-6f12-4f9a-88e8-63c8448d390f",
   "metadata": {},
   "source": [
    "To unload it type `launchctl unload filepath`\n",
    "\n",
    "To check all plist running in the pc, type `launchctl list`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c2f58-1155-47af-b1f7-5a6b13f3994c",
   "metadata": {},
   "source": [
    "### Since the first two method really doesn't work, there is a third method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b0863-d6b3-496b-a01f-676c9ac54474",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=nVlOapHc-kg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
