{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485abe24-976e-4bfc-9939-8f48e5817f7b",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "\n",
    "Resources: \n",
    "\n",
    "https://docs.scrapy.org/en/latest/\n",
    "\n",
    "https://www.youtube.com/watch?v=mBoX_JCKZTE\n",
    "\n",
    "https://thepythonscrapyplaybook.com/freecodecamp-beginner-course/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b058f9-6795-452c-a17a-afd13d36afcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf39e95-29c0-4e67-ae6d-37bed4c47d31",
   "metadata": {},
   "source": [
    "### Part 1: Initialize your project \n",
    "\n",
    "`scrapy startproject *projectname*` Import the framework (cd project directory)\n",
    "\n",
    "You will see the following files:\n",
    "\n",
    "`*project name*` folder<br>\n",
    "<ul>\n",
    "<li>`spiders` folder</li>\n",
    "<ul>\n",
    "<li>`__init__.py`</li>\n",
    "</ul>\n",
    "<li>`settings.py`</li>\n",
    "<li>`pipelines.py`</li>\n",
    "<li>`middleware.py`</li>\n",
    "<li>`items.py`</li>\n",
    "<li>`__init__.py`</li>\n",
    "</ul>\n",
    "`scrapy.cfg`\n",
    "<br>\n",
    "`scrapy genspider *name of spider* *link of website*` Create spider object (run the command inside the `*project name*` folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831c305-56be-46fa-b836-49ab898843bd",
   "metadata": {},
   "source": [
    "## Part 2: Building spider\n",
    "\n",
    "#### **IPython:**\n",
    "\n",
    "- a python shell where you can text your selectors\n",
    "\n",
    "`pip install ipython`  A shell that is just easier to read (add shell = ipython in settings in scrapy.cfg)\n",
    "\n",
    "`scrapy shell` Open ipython in terminal\n",
    "\n",
    "\n",
    "`fetch('*url*')` return an object stored under 'response'\n",
    "\n",
    "`response.css('article.product_pod')` return a list of selectors xpath of \\<article\\> tag with class=\"product_pod\"\n",
    "\n",
    "`book.css('h3 a').attrib['href']` return the link inside the \\<a> tag under h3 tag of book.css\n",
    "\n",
    "#### **Execution**\n",
    "\n",
    "`scrapy crawl *nameofspider*` execute the spider (execute it in the directory where you can see the spider folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d6ecc-96a9-49f4-8132-0e9cd7948df5",
   "metadata": {},
   "source": [
    "## Part 3: Item Pipeline\n",
    "\n",
    "Similar to pipeline in machine learning, it is mostly used as an organized and systematic way for data cleaning\n",
    "\n",
    "Remember to **uncomment** `ITEM_PIPELINES = {\n",
    "   'bookscraper.pipelines.BookscraperPipeline': 300,\n",
    "}`  inside settings.py if you are using pipelines.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71f566-a391-4b91-93a4-b1d10834d1cd",
   "metadata": {},
   "source": [
    "## Part 4: Saving Data to Files & Databases\n",
    "\n",
    "#### **Saving files with terminal**\n",
    "`scrapy crawl *spidername* -O *filename*` Save the file in the same directory in the file type you specified (write mode, overwrite existing data)\n",
    "\n",
    "`scrapy crawl *spidername* -o *filename*` Save the file in the same directory in the file type you specified (append mode)\n",
    "\n",
    "#### **Saving file in settings.py**\n",
    "\n",
    "Alternatively you can define it in settings.py instead of specifying -O *filename*\n",
    "<br>\n",
    "`FEEDS = {'booksdata.json': {'format': 'json'}}`\n",
    "<br>\n",
    "If you want to specify a specific filename/type for a particular spider, you can define  <br>\n",
    "`custom_settings = {FEEDS = {'booksdata.json': {'format': 'json'}}}` <br>\n",
    "on top of your bookspider.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8d312-e95a-4124-96a3-c2dbd37f0803",
   "metadata": {},
   "source": [
    "## **mysql**\n",
    "\n",
    "Ok hear me out boy, if you've never use terminal before this thing gonna drive you nuts, fortunately here are some few easy steps you need to follow to get mysql installed in your macos.\n",
    "\n",
    "*Full tutorial: https://www.youtube.com/watch?v=37nyT3U6hFI*\n",
    "\n",
    "1. Install mysql from https://dev.mysql.com/downloads/mysql/\n",
    "\n",
    "2. Open terminal, if you type mysql, you notice an error particularly **zsh command not found mysql** this is because mysql is not on the echo $PATH yet\n",
    "\n",
    "3. In terminal type `vi .zshrc` to navigate into vim editor\n",
    "\n",
    "4. Press 'i' to enter insert mode, go to a newline and type `export PATH=${PATH}:/usr/local/mysql/bin`, press 'esc' and press `:wq` which means write and quit the vim editor\n",
    "\n",
    "5. Now type `source .zshrc` in the terminal, if there are no errors you should be good to go\n",
    "\n",
    "6. To use mysql you can type `mysql -u root -p` and type your password\n",
    "\n",
    "\n",
    "To connect mysql to python you need to do **pip install mysql mysql-connector-python**\n",
    "\n",
    "Here are some useful mysql queries to initialize the database:\n",
    "\n",
    "`show databases;` (show all the databases) <br>\n",
    "`create database *database name*`<br>\n",
    "`use *database name*` (active the particular database) \n",
    "\n",
    "-------------------------------------------\n",
    "Ran into WHOLE BUNCH OF ERRORS when try to do **pip install mysql mysql-connector-python**, here are some solutions that might be useful:\n",
    "\n",
    "**If you download mysql from their sketchy official web and encounter some errors such as sql.h not found, do `brew install mysql` on your terminal**\n",
    "\n",
    "If you ran into some weird errors like pkg not found or something like that, try the follow:\n",
    "`export MYSQLCLIENT_CFLAGS=\"-I/usr/local/include/mysql\"` <br>\n",
    "`export MYSQLCLIENT_LDFLAGS=\"-L/usr/local/lib -lmysqlclient\"`\n",
    "--------\n",
    "\n",
    "#### Using mysql\n",
    "\n",
    "To use mysql, you will need to create a new class in **pipeline.py** *(see pipeline.py class SaveToMySQLPipeline for more)\n",
    "\n",
    "Also remember everytime you add a class to pipeline, add it to **settings.py** also (make sure it has a higher number than previous pipelines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee8ac9-b48f-44a3-97a9-61b097decc28",
   "metadata": {},
   "source": [
    "## Part 5: Bypassing Blocks\n",
    "\n",
    "**General rule of thumb**: If the data is publicly available and you do not need to login then it is most likely ok to scrape the website\n",
    "\n",
    "\n",
    "**User-Agent** (Inspect > Network > doc > click in a doc > header > user-agent)\n",
    "\n",
    "Visit https://useragentstring.com/ for detailed information of your user-agent\n",
    "\n",
    "Website mainly track who you are through **IP, cookies, flag/counter, useragent** and in general the **request header** (everything you send to the server when you make a request, user-agent is part of the request header).\n",
    "\n",
    "The idea of bypassing blocks is to change your request headers every time (eg. user-agent) to trick the antiscraping bot that you are a 'different' user. (Noted that you can set the user-agent in settings, ie USER_AGENT = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb4b37d-c279-484b-bff1-7bfd0fd2cc08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
