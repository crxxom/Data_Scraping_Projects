{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485abe24-976e-4bfc-9939-8f48e5817f7b",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "\n",
    "Resources: \n",
    "\n",
    "https://docs.scrapy.org/en/latest/\n",
    "\n",
    "https://www.youtube.com/watch?v=mBoX_JCKZTE\n",
    "\n",
    "https://thepythonscrapyplaybook.com/freecodecamp-beginner-course/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b058f9-6795-452c-a17a-afd13d36afcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf39e95-29c0-4e67-ae6d-37bed4c47d31",
   "metadata": {},
   "source": [
    "### Part 1: Initialize your project \n",
    "\n",
    "`scrapy startproject *projectname*` Import the framework (cd project directory)\n",
    "\n",
    "``scrapy genspider *name of spider* *link of website*`` Create spider object (run the command inside the `*project name*` folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831c305-56be-46fa-b836-49ab898843bd",
   "metadata": {},
   "source": [
    "## Part 2: Building spider\n",
    "\n",
    "#### **IPython:**\n",
    "\n",
    "- a python shell where you can text your selectors\n",
    "\n",
    "`pip install ipython`  A shell that is just easier to read (add shell = ipython in settings in scrapy.cfg)\n",
    "\n",
    "`scrapy shell` Open ipython in terminal\n",
    "\n",
    "\n",
    "`fetch('*url*')` return an object stored under 'response'\n",
    "\n",
    "`response.css('article.product_pod')` return a list of selectors xpath of \\<article\\> tag with class=\"product_pod\"\n",
    "\n",
    "`book.css('h3 a').attrib['href']` return the link inside the \\<a> tag under h3 tag of book.css\n",
    "\n",
    "#### **Execution**\n",
    "\n",
    "`scrapy crawl *nameofspider*` execute the spider (execute it in the directory where you can see the spider folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d6ecc-96a9-49f4-8132-0e9cd7948df5",
   "metadata": {},
   "source": [
    "## Part 3: Item Pipeline\n",
    "\n",
    "Similar to pipeline in machine learning, it is mostly used as an organized and systematic way for data cleaning\n",
    "\n",
    "Remember to **uncomment** `ITEM_PIPELINES = {\n",
    "   'bookscraper.pipelines.BookscraperPipeline': 300,\n",
    "}`  inside settings.py if you are using pipelines.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71f566-a391-4b91-93a4-b1d10834d1cd",
   "metadata": {},
   "source": [
    "## Part 4: Saving Data to Files & Databases\n",
    "\n",
    "#### **Saving files with terminal**\n",
    "`scrapy crawl *spidername* -O *filename*` Save the file in the same directory in the file type you specified (write mode, overwrite existing data)\n",
    "\n",
    "`scrapy crawl *spidername* -o *filename*` Save the file in the same directory in the file type you specified (append mode)\n",
    "\n",
    "#### **Saving file in settings.py**\n",
    "\n",
    "Alternatively you can define it in settings.py instead of specifying -O *filename*\n",
    "<br>\n",
    "`FEEDS = {'booksdata.json': {'format': 'json'}}`\n",
    "<br>\n",
    "If you want to specify a specific filename/type for a particular spider, you can define  <br>\n",
    "`custom_settings = {FEEDS = {'booksdata.json': {'format': 'json'}}}` <br>\n",
    "on top of your bookspider.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8d312-e95a-4124-96a3-c2dbd37f0803",
   "metadata": {},
   "source": [
    "## **mysql**\n",
    "\n",
    "Ok hear me out boy, if you've never use terminal before this thing gonna drive you nuts, fortunately here are some few easy steps you need to follow to get mysql installed in your macos.\n",
    "\n",
    "*Full tutorial: https://www.youtube.com/watch?v=37nyT3U6hFI*\n",
    "\n",
    "1. Install mysql from https://dev.mysql.com/downloads/mysql/\n",
    "\n",
    "2. Open terminal, if you type mysql, you notice an error particularly **zsh command not found mysql** this is because mysql is not on the echo $PATH yet\n",
    "\n",
    "3. In terminal type `vi .zshrc` to navigate into vim editor\n",
    "\n",
    "4. Press 'i' to enter insert mode, go to a newline and type `export PATH=${PATH}:/usr/local/mysql/bin`, press 'esc' and press `:wq` which means write and quit the vim editor\n",
    "\n",
    "5. Now type `source .zshrc` in the terminal, if there are no errors you should be good to go\n",
    "\n",
    "6. To use mysql you can type `mysql -u root -p` and type your password\n",
    "\n",
    "\n",
    "To connect mysql to python you need to do **pip install mysql mysql-connector-python**\n",
    "\n",
    "Here are some useful mysql queries to initialize the database:\n",
    "\n",
    "`show databases;` (show all the databases) <br>\n",
    "`create database *database name*`<br>\n",
    "`use *database name*` (active the particular database) \n",
    "\n",
    "-------------------------------------------\n",
    "Ran into WHOLE BUNCH OF ERRORS when try to do **pip install mysql mysql-connector-python**, here are some solutions that might be useful:\n",
    "\n",
    "**If you download mysql from their sketchy official web and encounter some errors such as sql.h not found, do `brew install mysql` on your terminal**\n",
    "\n",
    "If you ran into some weird errors like pkg not found or something like that, try the follow:\n",
    "`export MYSQLCLIENT_CFLAGS=\"-I/usr/local/include/mysql\"` <br>\n",
    "`export MYSQLCLIENT_LDFLAGS=\"-L/usr/local/lib -lmysqlclient\"`\n",
    "--------\n",
    "\n",
    "#### Using mysql\n",
    "\n",
    "To use mysql, you will need to create a new class in **pipeline.py** *(see pipeline.py class SaveToMySQLPipeline for more)\n",
    "\n",
    "Also remember everytime you add a class to pipeline, add it to **settings.py** also (make sure it has a higher number than previous pipelines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee8ac9-b48f-44a3-97a9-61b097decc28",
   "metadata": {},
   "source": [
    "## Part 5: Bypassing Blocks\n",
    "\n",
    "**General rule of thumb**: If the data is publicly available and you do not need to login then it is most likely ok to scrape the website\n",
    "\n",
    "\n",
    "**User-Agent** (Inspect > Network > doc > click in a doc > header > user-agent)\n",
    "\n",
    "Visit https://useragentstring.com/ for detailed information of your user-agent\n",
    "\n",
    "Website mainly track who you are through **IP, cookies, flag/counter, useragent** and in general the **request header** (everything you send to the server when you make a request, user-agent is part of the request header).\n",
    "\n",
    "The idea of bypassing blocks is to change your request headers every time (eg. user-agent) to trick the antiscraping bot that you are a 'different' user. (Noted that you can set the user-agent in settings, ie USER_AGENT = '')\n",
    "\n",
    "#### Faking user-agent\n",
    "\n",
    "1. Create a list of user_agents manually inside bookspider.py and run `headers={\"User-Agent\": self.user_agent_list(random.randint(0, len(self.user_agent_list)-1))}` behind every request (yield) that you make\n",
    "\n",
    "2. Use middleware https://scrapeops.io/app/headers to generate fake user agents in middleware.py. Make sure to enable *DOWNLOADER_MIDDLEWARES* settings in settings.py just like pipeline.\n",
    "\n",
    "In settings you may see a line that says `ROBOTSTXT_OBEY = True`. Basically each time you scrape a website, the first thing scrapy will do is check the robots.txt rules that most websites contains that specify whether the side is open for scraping, which pages are not allowed to be scrape etc. If you set it to True, scrapy will follow all the instructions in robots.txt, but usually you should turn it off especially you are scraping more complex and large websites like amazon etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488bcd1-54a2-4939-9a8f-2111345b66ba",
   "metadata": {},
   "source": [
    "## Part 6: Rotating Proxy and Proxy API\n",
    "\n",
    "In our previous section we manage to change our user-agent and request headers, but in more complex and big websites, they will be able to detect your IP address as well, that's why we need proxy to rotate and help hide our identity\n",
    "\n",
    "1. **Use free proxy lists online** <br>\n",
    "    Downsides:\n",
    "    - Since so many people are using them, the request time may be slow\n",
    "    - These free IP may very likely already be blocked by popular websites since it's open for eveyone to use and some people probably already tried and get blocked\n",
    "    <br><br>\n",
    "    \n",
    "    How to use:\n",
    "    `pip install scrapy-rotating-proxies`\n",
    "    Set up the proxy list in **settings.py** and remember to add it to the downloader middleware\n",
    "    \n",
    "2. **Use proxy providers** <br>\n",
    "    Basically it's similar to free proxy list but these providers provide millions are quality ip address and handle everything under the hood for you. Downside is that it's paid access. Eg. https://smartproxy.com/ See 2:53:00 on https://www.youtube.com/watch?v=mBoX_JCKZTE for more on how to set it up\n",
    "    \n",
    "3. **Proxy API**\n",
    "    An API proxy is a thin application program interface (API) server that exposes an interface for an existing service or services. A \"proxy\" is something that acts as an agent or intermediary for something else.\n",
    "    Basically when you send a request it will first go through this service and bypass the blockers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a2089-e2b5-43a1-9115-8a7bf647d3bf",
   "metadata": {},
   "source": [
    "## Part 7: Run Spiders in cloud with Scrapyd\n",
    "\n",
    "1. ScrapyD - Free & open source, need your own server, configuration needed to set up, no scheduler, controlled via API, optional UI via 3rd parties available\n",
    "\n",
    "2. ScrapeOps - Free, easy to set up, need your own server, good graphical UI, built in jobs/spider monitoring\n",
    "\n",
    "3. ScrapyCloud - Paid, easiest to set up, easy to scale, no servers needed, good integrations with other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1233275-1048-4994-ab2e-289fd30177d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
